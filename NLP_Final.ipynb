{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Title Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "# import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pprint as pp\n",
    "from scipy import spatial\n",
    "from TranslationTool.langconv import *\n",
    "from hanziconv import HanziConv\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models\n",
    "**Don't run this block twice!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "def loading():\n",
    "    # load stopwords\n",
    "    with open(\"stopwords.txt\", encoding='utf8') as fp:\n",
    "        dat = fp.read()\n",
    "    global stop_words\n",
    "    stop_words = dat.split('\\n')\n",
    "\n",
    "    # load word2vec model\n",
    "    global model\n",
    "    # model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary = True, unicode_errors = 'ignore')\n",
    "    model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "\n",
    "    # load idf list\n",
    "    global idf\n",
    "    with open('idf.txt', encoding='utf8') as fp:\n",
    "        dat = fp.read()\n",
    "    lines = dat.split('\\n')\n",
    "    del lines[-1]\n",
    "    idf = [(l.split()[0], float(l.split()[1])) for l in lines]\n",
    "    \n",
    "    # connect to StanfordCoreNLP\n",
    "    global nlp\n",
    "    nlp = StanfordCoreNLP('http://140.113.193.76:9000')\n",
    "    \n",
    "    # feature database of evaluation part\n",
    "    global f1_dict, mv_list_vec, f3_dict\n",
    "    f1_dict = pickle.load(open(\"f1_dict.pkl\", \"rb\"))\n",
    "    mv_list_vec = pickle.load(open(\"f2_dict.pkl\", \"rb\"))\n",
    "    f3_dict = pickle.load(open(\"f3_dict.pkl\", \"rb\"))\n",
    "    \n",
    "    return\n",
    "\n",
    "# global variable with empty value as initialization\n",
    "model = None\n",
    "# model_path = \"cna_asbc_cbow_d300_w10_n10_hs0_i15.vectors.bin\"\n",
    "nlp = None\n",
    "\n",
    "stop_words = []\n",
    "idf = []\n",
    "\n",
    "f1_dict = {}\n",
    "mv_list_vec = []\n",
    "f3_dict = {}\n",
    "\n",
    "# loading\n",
    "loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfGen(path):\n",
    "    \n",
    "    with open(path, encoding='utf8') as fp:\n",
    "        dat = fp.read()\n",
    "    \n",
    "    lines = dat.split('\\n')\n",
    "    del lines[-1]\n",
    "    \n",
    "    # segmentation\n",
    "    segs = []\n",
    "    for line in lines:\n",
    "        tmp = [Converter('zh-hant').convert(w) for w in line.split('/')[:-1]]\n",
    "        segs.extend(tmp)\n",
    "        # jieba.cut(line, cut_all=False)\n",
    "    \n",
    "    # compute tf (word count and frequency)\n",
    "    words, counts = np.unique(segs, return_counts=True) # default: axis=None\n",
    "    frequency = counts / len(segs)\n",
    "    tf = list(zip(words, frequency))\n",
    "    tf = sorted(tf, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return tf\n",
    "\n",
    "def tfidfGen(tf):\n",
    "    \n",
    "    idf_dict = {word: value for word, value in idf}\n",
    "    \n",
    "    words = []\n",
    "    values = []\n",
    "    for word, value in tf:\n",
    "        words.append(word)\n",
    "        values.append(value * idf_dict[word])\n",
    "\n",
    "    tfidf = list(zip(words, values))\n",
    "    tfidf = sorted(tfidf, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordExt(tfidf):\n",
    "    \n",
    "    global model, stop_words, num_keywords\n",
    "    \n",
    "    i = 0\n",
    "    word_ls = []\n",
    "    for word, count in tfidf:\n",
    "        if i == num_keywords:\n",
    "            break\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word not in model:\n",
    "            continue\n",
    "        word_ls.append(word)\n",
    "        i += 1\n",
    "    \n",
    "    return word_ls\n",
    "\n",
    "def keywordSel(word_ls):\n",
    "    \n",
    "    # remove NR\n",
    "    global nlp\n",
    "    \n",
    "    new_word_ls = []\n",
    "    for word in word_ls:\n",
    "        output = nlp.annotate(word, properties={\n",
    "            'annotators': 'pos',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "        pos = output['sentences'][0]['tokens'][0]['pos']\n",
    "        if pos != 'NR':\n",
    "            new_word_ls.append((word, pos))\n",
    "    \n",
    "    return new_word_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Classification (feature base, which is not used now)\n",
    " - feature generation\n",
    " - SVM classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable with predefined value\n",
    "num_keywords = 100\n",
    "\n",
    "def featureGen(word_ls):\n",
    "    \n",
    "    # add feature from word2vec\n",
    "    \n",
    "    feature = list(np.zeros(300))\n",
    "    for word in word_ls:\n",
    "        feature += model.word_vec(word)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def featureGen2(tfidf):\n",
    "    \n",
    "    global word_ls\n",
    "    \n",
    "    tfidf_dict = {word: value for word, value in tfidf}\n",
    "    feature = []\n",
    "    for word in word_ls:\n",
    "        if word in tfidf_dict:\n",
    "            feature.append(tfidf_dict[word])\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    \n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Classification\n",
    " - rule base\n",
    " - with corresponding to keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable with predefined value\n",
    "# keyword definition: [[keyword, ...], [key alphabet, ...]]\n",
    "script_keyword = {\n",
    "    'action': [['計畫', '特工', '殺手'], ['警', '賭']],\n",
    "    'comedy': [['嘿咻'], ['裸', '妓', '屁']],\n",
    "    'crime': [['暴力', '受害者', '罪犯', '犯罪'], ['毒']],\n",
    "    'drama': [[], []],\n",
    "    'fantasy': [[], ['獸']],\n",
    "    'horror': [[], ['魔', '怪', '屍']],\n",
    "    'romance': [[], ['愛']],\n",
    "    'sci_fi': [['星球', '星際', '太空', '時空', '星艦'], []],\n",
    "    'war': [['坦克', '地雷'], ['軍', '戰']],\n",
    "}\n",
    "\n",
    "\n",
    "def ruleBaseClassify(word_ls):\n",
    "    # print(word_ls)\n",
    "    for key, content in script_keyword.items():\n",
    "        if content[0] == []:\n",
    "            continue\n",
    "        for sw in content[0]:\n",
    "            if sw in word_ls:\n",
    "                return key\n",
    "    \n",
    "    word_ls_str = ''.join(word_ls)\n",
    "    for key, content in script_keyword.items():\n",
    "        if content[1] == []:\n",
    "            continue\n",
    "        for sw in content[1]:\n",
    "            if sw in word_ls_str:\n",
    "                return key\n",
    "    \n",
    "    return 'drama'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable with predefined value\n",
    "# rule definition: [POS, SPECIAL_WORD, ReverseOrNot]\n",
    "special_rule = {\n",
    "    'action': [['NN', '玩命', True], ['NN', '啟動', False], ['NN', '神鬼', True], ['NN', '遊戲', True]],\n",
    "    'comedy': [['NN', '行不行', False]],\n",
    "    'crime': [['NN', '檔案', False], ['NN', '風暴', False], ['NN', '風雲', False]],\n",
    "    'drama': [],\n",
    "    'fantasy': [],\n",
    "    'horror': [['NN', '絕命', True], ['NN', '失控', True], ['VV', '鬼', True]],\n",
    "    'romance': [['NN', '真愛', True]],\n",
    "    'sci_fi': [['NN', '星際', True], ['NN', '世界', False]],\n",
    "    'war': [['NN', '重生', False]],\n",
    "}\n",
    "\n",
    "def block(s1, s2):\n",
    "    \n",
    "    # too similar\n",
    "    if model.wv.similarity(s1, s2) > 0.5:\n",
    "        # print('Too similar: %s, %s' %(s1, s2))\n",
    "        return True\n",
    "    \n",
    "    # contain the same alphabet\n",
    "    for ele in s1:\n",
    "        if ele in s2:\n",
    "            # print('Contain the same alphanet: %s, %s' %(s1, s2))\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def titleCanGen(genre, word_pos_ls):\n",
    "    \n",
    "    global special_rule\n",
    "    \n",
    "    pos_dict = {}\n",
    "    for word, pos in word_pos_ls:\n",
    "        try:\n",
    "            pos_dict[pos].append(word)\n",
    "        except:\n",
    "            pos_dict[pos] = [word]\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for rule in special_rule[genre]:\n",
    "        pos = rule[0]\n",
    "        for word in pos_dict[pos]:\n",
    "            if block(rule[1], word):\n",
    "                continue\n",
    "            if rule[2]:\n",
    "                candidates.append(rule[1]+word)\n",
    "            else:\n",
    "                candidates.append(word+rule[1])\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable with predefined value\n",
    "special_word = [HanziConv.toSimplified(rule[1]) for key, content in special_rule.items() for rule in content]\n",
    "pos_simpify_dic = {'NN': 'N', 'NR': 'N', 'NT': 'N', 'VE': 'V', 'VV': 'V'}\n",
    "\n",
    "############################## parse.py ##############################\n",
    "\n",
    "def Simpify(pos):\n",
    "    global pos_simpify_dic\n",
    "    if pos in pos_simpify_dic.keys():\n",
    "        return pos_simpify_dic[pos]\n",
    "    return pos\n",
    "\n",
    "def Parse_String(mvname):\n",
    "    parse_results = []\n",
    "    # convert to simple chinese\n",
    "    text = HanziConv.toSimplified(mvname)\n",
    "    # parse by core nlp\n",
    "    global nlp\n",
    "    output = nlp.annotate(text, properties={'annotators': 'tokenize, ssplit, pos', 'outputFormat': 'json'})\n",
    "    parse_results.append(output)\n",
    "    return parse_results\n",
    "\n",
    "def Get_Parse_Result(parse_results):\n",
    "    # print(parse_results[0]['sentences'][0]['parse'])\n",
    "    l = []\n",
    "    for i in range(len(parse_results)):\n",
    "        for word in parse_results[i]['sentences'][0]['tokens']:\n",
    "            global special_word\n",
    "            if word['word'] in special_word:\n",
    "                l.append((word['word'], 'N'))\n",
    "            else:\n",
    "                l.append((word['word'], Simpify(word['pos'])))\n",
    "    return l\n",
    "\n",
    "def POStag(name):\n",
    "    parse_results = Parse_String(name)\n",
    "    l = Get_Parse_Result(parse_results)\n",
    "    return l\n",
    "\n",
    "############################## feature2.py ##############################\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    res = 1 - spatial.distance.cosine(v1, v2)\n",
    "    return res\n",
    "\n",
    "def Create_mv_vector(mv_name):\n",
    "    # logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # model = models.Word2Vec.load('med250.model.bin')\n",
    "    global model\n",
    "    \n",
    "    sum_vec = [0] * 250\n",
    "    for word in mv_name:\n",
    "        try:\n",
    "            v = model.wv[word]\n",
    "            sum_vec = sum_vec + v\n",
    "        except KeyError:\n",
    "            sum_vec = sum_vec\n",
    "    \n",
    "    return sum_vec\n",
    "\n",
    "def Most_similar(mv_name):\n",
    "    mv_name = [HanziConv.toTraditional(x) for x in mv_name]\n",
    "    mv_vec = Create_mv_vector(mv_name)\n",
    "\n",
    "    # caculate mv_name_vector\n",
    "    global mv_list_vec\n",
    "    # mv_list_vec = pickle.load(open(\"f2_dict.pkl\", \"rb\"))\n",
    "    \n",
    "    # Similarity with those movies\n",
    "    cosine_list = []\n",
    "    for mv in mv_list_vec:\n",
    "        cosine_list.append(cosine(mv_vec, mv))\n",
    "        # print(cosine(mv_vec, mv))\n",
    "    return max(cosine_list)\n",
    "\n",
    "############################## main.py ##############################\n",
    "\n",
    "def evaluation(mv_name):\n",
    "\n",
    "    # print(mv_name)\n",
    "    mv_name = HanziConv.toSimplified(mv_name)\n",
    "    \n",
    "    global f1_dict, f3_dict, model\n",
    "    \n",
    "    # POS tag\n",
    "    parse_result = POStag(mv_name)\n",
    "    # print(parse_result)\n",
    "\n",
    "    # f1 score\n",
    "    mv_words = [x[0] for x in parse_result]\n",
    "    pos_form = [x[1] for x in parse_result]\n",
    "    pos_form = tuple(pos_form)\n",
    "    \n",
    "    if pos_form in f1_dict:\n",
    "        f1 = f1_dict[pos_form]\n",
    "    else:\n",
    "        f1 = 0\n",
    "\n",
    "    # f2 score\n",
    "    f2 = Most_similar(mv_words)\n",
    "    if np.isnan(f2):\n",
    "        f2 = 0\n",
    "\n",
    "    # f3 score\n",
    "    # words = [x[0] for x in parse_result]\n",
    "    f3 = 0\n",
    "    for word in mv_words:\n",
    "        if word in f3_dict:\n",
    "            f3 = f3 + 0.2 + f3_dict[word]\n",
    "        else:\n",
    "            f3 = 0\n",
    "\n",
    "    # print(\"f1: %f, f2: %f, f3: %f\" %(f1, f2, f3))\n",
    "    score = f1 + f2 + f3\n",
    "    # print(\"score = \", score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Generation\n",
    " - main process of title generation\n",
    " - input: a file path\n",
    " - output: a sorted title list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titleGen(path):\n",
    "    # tfidf\n",
    "    tf = tfGen(path)\n",
    "    tfidf = tfidfGen(tf)\n",
    "\n",
    "    # keyword\n",
    "    word_ls = keywordExt(tfidf)\n",
    "    word_pos_ls = keywordSel(word_ls)\n",
    "    word_ls = [word for word, pos in word_pos_ls]\n",
    "    # pp.pprint(word_ls)\n",
    "\n",
    "    # get genre\n",
    "    genre = ruleBaseClassify(word_ls)\n",
    "    # print(genre)\n",
    "\n",
    "    # title candidate generation\n",
    "    title_candidates = titleCanGen(genre, word_pos_ls)\n",
    "    # pp.pprint(title_candidates)\n",
    "\n",
    "    # evaluation\n",
    "    title_score = [(title, evaluation(title)) for title in title_candidates]\n",
    "    title_score = sorted(title_score, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return title_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 Main Process\n",
    "Put all the testing script file in one folder, and then find the best title for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if sys.path[0] == '':\n",
      "c:\\python35\\lib\\site-packages\\scipy\\spatial\\distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "c:\\python35\\lib\\site-packages\\scipy\\spatial\\distance.py:505: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afanda.txt ('外星人世界', 1.4552664329468497)\n",
      "La La Land.txt ('失控鼓手', 1.2002845092955547)\n"
     ]
    }
   ],
   "source": [
    "# folder path\n",
    "folder = \"test\"\n",
    "files = os.listdir(folder)\n",
    "\n",
    "for file in files:\n",
    "    path = folder + '\\\\' + file\n",
    "    title_score = titleGen(path)\n",
    "    print(file, title_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
